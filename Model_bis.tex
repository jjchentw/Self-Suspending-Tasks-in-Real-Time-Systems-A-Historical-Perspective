\mychapter{Examples of Self-Suspending Task Systems}
\label{sec:examples}

Self-suspensions arise in real-time systems for a range of reasons. To motivate the need for suspension-aware analysis, we initially review three common causes. 


{\bf Example 1: I/O- or Memory-Intensive Tasks.}  \hspace{0.1in}
An I/O-intensive task may have to use DMA (Direct Memory Access) to transfer a large amount of data to or from peripheral devices. This can take from a few microseconds up to milliseconds. In such cases, a job of a task executes for a certain amount of time, then initiates an I/O activity, and suspends itself. When the I/O activity completes, the job can be moved back to the ready queue to be (re)-eligible for execution. 

This also applies to systems with scratchpad memories, where the scratchpad memory allocated to a task is dynamically updated during its execution. In such a case, a job of a task executes for a certain amount of time, then initiates a scratchpad memory update to push its content from the scratchpad memory to the main memory and to pull some content from the main memory to the scratchpad memory, often using DMA. During the DMA transfers to update the scratchpad memory, the job suspends itself. Such memory access latency can become much more dynamic and larger when we consider multicore platforms with shared memory, due to bus contention and competition for memory resources.

{\bf Example 2: Multiprocessor Synchronization.} \hspace{0.1in}
Under a suspension-based locking protocol, tasks that are denied access to a shared resource (\ie, that block on a lock) are suspended. Interestingly, on uniprocessors, the resulting suspensions can be accounted for more efficiently than general self-suspensions by considering the blocking time due to the lower-priority job(s) that hold(s) the required shared resource(s). More detailed discussions about the reason why uniprocessor synchronization does not have to be considered to be self-suspension can be found in Section~\ref{sec:sem-uni}. In multiprocessor systems, self-suspensions can arise (for instance) under partitioned scheduling (in which each task is assigned statically on a dedicated processor) when the tasks have to synchronize their access to shared resources (\eg, shared I/O devices, communication buffers, or scheduler locks) with suspension-based locks (\eg, binary semaphores). 

We use a binary semaphore shared by two tasks assigned on two different processors as an example. Suppose each of these two tasks has a critical section protected by the semaphore. If one of them, say task $\tau_1$, is using the semaphore on the first processor and another task, say $\tau_2$, executing on the second processor intends to enter its critical section, then task $\tau_2$ has to wait until the critical section of task $\tau_1$ finishes on the first processor. During the execution of task $\tau_1$'s critical section, task $\tau_2$ \emph{suspends} itself. 

% There are several synchronization protocols in the literature, some of them prevent the above self-suspending behavior by using \emph{spin-locking} by pessimistically increasing the worst-case execution time. Some of them try to utilize the unused resource on the second processor in the above example by running other jobs. 

In this paper, we will specifically examine the existing results for multiprocessor synchronization protocols in \mysectionref{}~\ref{sec:syn}. %Such problems have been specifically studied in \cite{rajkumar-1990,lakshmanan-2009,zeng-2011,bbb-2013,yang-2013,kim-2014,han-2014,carminati-2014,yang-2014}. 

{\bf Example 3: Hardware Acceleration by Using Co-Processors and Computation Offloading.} \hspace{0.1in}
 In many embedded systems, selected portions of programs are preferably (or even necessarily) executed on dedicated hardware co-processors to satisfy performance requirements.  Such co-processors include for instance application-specific integrated circuits (ASICs), digital signal processors (DSPs), field-programmable gate arrays (FPGAs), graphics processing units (GPUs), etc. There are two typical strategies for utilizing hardware co-processors. One is busy-waiting, in which the software task does not give up its privilege on the processor and has to wait by spinning on the processor until the co-processor finishes the requested work (see Figure~\ref{fig:example-fpga}(b) for an example). Another is to suspend the software task. This strategy frees the processor so that it can be used by other ready tasks. Therefore, even in single-CPU systems more than one task may be simultaneously executed in computation: one task executing on the processor and others on each of the available co-processors. This arrangement is called \emph{limited parallelism} \cite{RTAS-AudsleyB04}, which improves the performance by effectively utilizing the processor and the co-processors, as shown in Figure~\ref{fig:example-fpga}(a).

% Another domain which deploys hardware acceleration is that of General-Purpose Graphics Processor (GPGPU) computing. 
% Unlike FPGAs which are used for the synthesis of custom co-processors, this paradigm uses standard PC hardware as an accelerator.
% Graphic Processor Units (GPUs) are massively parallel architectures, which offer immense processing power, compared to CPUs. 
% Traditionally they were only used for the processing of computer graphics. This changed with the advent of programming
% models such as CUDA (Compute Unified Device Architecture) which facilitate harnessing that power for general-purpose computing 
% applications characterized by inherent parallelism. CUDA functions are implemented as hundreds or thousands of ultra-light 
% (single-cycle context switch) GPU threads, each performing a different part of the overall computation.
% For many kinds of applications, \eg, scientific, or floating-point heavy, with minimal data dependencies among 
% threads, the speed-up is by tens or hundreds of times.

% In the typical setup, the GPU is used as a co-processor. The native application (\eg, x86) running on the host
% copies the input from the main memory to the GPU memory, and then launches the CUDA application (called ``kernel") on the GPU;
% Upon completion, the output is copied from GPU memory to main memory. As with FPGA co-processors, it makes sense to 
% have the host application suspend, for the duration of the CUDA kernel. 


\begin{figure}[t]
  \centering
\def\uxfpga{0.3cm} 
\subfloat[Using several FPGAs in parallel (with self-suspensions).]{
    \scalebox{0.8}{
      \begin{tikzpicture}[x=\uxfpga,y=\uy,auto, thick]
        \draw[->] (0,0) -- coordinate (xaxis) (45,0) node[anchor=north]{$t$};
       \node[anchor=east] at (0, 0.75) {CPU};
       \node[anchor=east] at (0, 2.25) {HW 1};
       \node[anchor=east] at (0, 3.75) {HW 2};
       \node[anchor=east] at (0, 5.25) {HW 3};
       \draw (0, 0) -- (0, 6);
       \foreach \x in {0,2,...,42}{
         \draw[-,below](\x,0) -- (\x,-0.3)
         node[] {\pgfmathtruncatemacro\yi{\x} \yi};
       }

       \draw[->](0, 6.5) -- (0, 7.2) node[anchor=south, xshift=-0.4cm]{\footnotesize $\tau_1$ arrives};
       \draw[->](4, 6.5) -- (4, 7.2) node[anchor=south]{\footnotesize $\tau_2$ arrives};
       \draw[->](8, 6.5) -- (8, 7.2) node[anchor=south, xshift=0.4cm]{\footnotesize $\tau_3$ arrives};

         \node[task7, minimum width=4*\uxfpga, anchor=south west] at (0, 0){\footnotesize $\tau_1$};         
         \node[task7, minimum width=3*\uxfpga, anchor=south west] at (4, 0){\footnotesize $\tau_2$};         
         \node[task7, minimum width=2*\uxfpga, anchor=south west] at (8, 0){\footnotesize $\tau_3$};         
         \node[task7, minimum width=6*\uxfpga, anchor=south west] at (10, 0){\footnotesize $\tau_1$};         
         \node[task7, minimum width=2*\uxfpga, anchor=south west] at (16, 0){\footnotesize $\tau_2$};         
         \node[task7, minimum width=2*\uxfpga, anchor=south west] at (18, 0){\footnotesize $\tau_3$};         
         \node[task7, minimum width=2*\uxfpga, anchor=south west] at (24, 0){\footnotesize $\tau_3$};         

        \node[task9, minimum width=6*\uxfpga, anchor=south west] at (4, 1.5){\footnotesize $\tau_1$};         
        \node[task9, minimum width=9*\uxfpga, anchor=south west] at (7, 3){\footnotesize $\tau_2$};         
        \node[task9, minimum width=4*\uxfpga, anchor=south west] at (20, 4.5){\footnotesize $\tau_3$};         


         \node[](e1) at (4.2, 0.8){};
         \node[](s1) at (4.2, 2.8){};
         \node[](r1) at (9.9, 2.8){};
         \node[](l1) at (9.9, 0.8){};
         \path[->,>=stealth'] (e1)[anchor=center] edge[bend left] node[anchor=center]{} (s1);
         \path[->,>=stealth'] (r1)[anchor=center] edge[bend left] node[anchor=center]{} (l1);

        \node[](e2) at (7.2, 0.8){};
         \node[](s2) at (7.2, 3.8){};
         \node[](r2) at (15.9, 3.8){};
         \node[](l2) at (15.9, 0.8){};
         \path[->,>=stealth'] (e2)[anchor=center] edge[bend left] node[anchor=center]{} (s2);
         \path[->,>=stealth'] (r2)[anchor=center] edge[bend left] node[anchor=center]{} (l2);
  
         \node[](e3) at (20.2, 0.8){};
         \node[](s3) at (20.2, 5.3){};
         \node[](r3) at (23.9, 5.3){};
         \node[](l3) at (23.9, 0.8){};
         \path[->,>=stealth'] (e3)[anchor=center] edge[bend left] node[anchor=center]{} (s3);
         \path[->,>=stealth'] (r3)[anchor=center] edge[bend left] node[anchor=center]{} (l3);
  
 \end{tikzpicture}} }

\subfloat[Serialized FPGA use (busy waiting).]{
   \scalebox{0.8}{
      \begin{tikzpicture}[x=\uxfpga,y=\uy,auto, thick]
        \draw[->] (0,0) -- coordinate (xaxis) (45,0) node[anchor=north]{$t$};
       \node[anchor=east] at (0, 0.75) {CPU};
       \node[anchor=east] at (0, 2.25) {HW 1};
       \node[anchor=east] at (0, 3.75) {HW 2};
       \node[anchor=east] at (0, 5.25) {HW 3};
       \draw (0, 0) -- (0, 6);
       \foreach \x in {0,2,...,42}{
         \draw[-,below](\x,0) -- (\x,-0.3)
         node[] {\pgfmathtruncatemacro\yi{\x} \yi};
       }
         \node[task7, minimum width=4*\uxfpga, anchor=south west] at (0, 0){\footnotesize $\tau_1$};         

         \node[task7, minimum width=6*\uxfpga, anchor=south west] at (10, 0){\footnotesize $\tau_1$};         

         \node[task7, minimum width=3*\uxfpga, anchor=south west] at (16, 0){\footnotesize $\tau_2$};         

         \node[task7, minimum width=2*\uxfpga, anchor=south west] at (28, 0){\footnotesize $\tau_2$};         

         \node[task7, minimum width=4*\uxfpga, anchor=south west] at (30, 0){\footnotesize $\tau_3$};         

         \node[task7, minimum width=2*\uxfpga, anchor=south west] at (38, 0){\footnotesize $\tau_3$};         

        \node[task9, minimum width=6*\uxfpga, anchor=south west] at (4, 1.5){\footnotesize $\tau_1$};         
        \node[task9, minimum width=9*\uxfpga, anchor=south west] at (19, 3){\footnotesize $\tau_2$};         
        \node[task9, minimum width=4*\uxfpga, anchor=south west] at (34, 4.5){\footnotesize $\tau_3$};         


         \node[](e1) at (4.2, 0.8){};
         \node[](s1) at (4.2, 2.8){};
         \node[](r1) at (9.9, 2.8){};
         \node[](l1) at (9.9, 0.8){};
         \path[->,>=stealth'] (e1)[anchor=center] edge[bend left] node[anchor=center]{} (s1);
         \path[->,>=stealth'] (r1)[anchor=center] edge[bend left] node[anchor=center]{} (l1);

        \node[](e2) at (19.2, 0.8){};
         \node[](s2) at (19.2, 3.8){};
         \node[](r2) at (27.9, 3.8){};
         \node[](l2) at (27.9, 0.8){};
         \path[->,>=stealth'] (e2)[anchor=center] edge[bend left] node[anchor=center]{} (s2);
         \path[->,>=stealth'] (r2)[anchor=center] edge[bend left] node[anchor=center]{} (l2);
  
         \node[](e3) at (34.2, 0.8){};
         \node[](s3) at (34.2, 5.3){};
         \node[](r3) at (37.9, 5.3){};
         \node[](l3) at (37.9, 0.8){};
         \path[->,>=stealth'] (e3)[anchor=center] edge[bend left] node[anchor=center]{} (s3);
         \path[->,>=stealth'] (r3)[anchor=center] edge[bend left] node[anchor=center]{} (l3);
  
         \node[task10, minimum width=6*\uxfpga, anchor=south west] at (4, 0){\footnotesize busy};         
         \node[task10, minimum width=9*\uxfpga, anchor=south west] at (19, 0){\footnotesize busy};         
         \node[task10, minimum width=4*\uxfpga, anchor=south west] at (34, 0){\footnotesize busy};         

 \end{tikzpicture}}       }
  \caption{An example of using FPGA for acceleration.}
  \label{fig:example-fpga}
\end{figure}


%{\bf Example 4: Computation Offloading}
Since modern embedded systems are designed to execute complicated applications, the limited resources, such as the battery capacity, the memory size, and the processor speed, may not satisfy the required computation demand. Offloading heavy computation to some powerful computing servers has been shown as an attractive solution, including optimizations for system performance and energy saving.
Computation offloading with real-time constraints has been specifically studied in two categories. In the first category, computation offloading always takes place at the end of a job and the post-processing time to process the result from the computing server is negligible. Such offloading scenarios do not incur self-suspending behavior  \cite{nimmagadda2010real,DBLP:conf/ecrts/TomaC13}. In the second category, non-negligible computation time after computation offloading is needed. For example, the computation offloading model studied in \cite{Liu_2014} defines three segments of a task: (1) the first segment is the local computation time to encrypt, extract, or compress the data, (2) the second segment is the worst-case waiting time to receive the result from the computing server, and (3) the third segment is either the local compensation if the result from the computing server is not received in time or the post processing if the result from the computing server is received in time. 
%Another similar model for soft real-time systems is adopted by Liu et al. \cite{DBLP:conf/ecrts/LiuLZGH015} by assuming that the offloading results are always received within a specified amount of time.

% \begin{figure}[t]
%   \centering
% \scalebox{0.85}{
%   \begin{tikzpicture}
%   \draw[thick][->] (4,0) -- (14.5,0) node[anchor=north]{$t$};
%   \draw[thick][->] (4,0) -- (4,0.8);
%   \draw[thick][->] (14,0.8) -- (14,0);
%   \draw[thick][<->] (7.1,0.3) -- (9.1, 0.3) node[anchor=south]{offloading (failed)}-- (11.1,0.3);
%   \node[task7,minimum width=2.8cm,anchor=south west](a) at (4.3,0) {\tiny pre-processing};
%   \node[task7,minimum width=1cm,anchor=south west](d) at (11.1,0) {\tiny compensation.};
%   \node(b1) at (2.2,0.2) {\footnotesize $Local$ $Compensation$};
  
  
%   \draw[thick][->] (4,1) -- (14.5,1) node[anchor=north]{$t$};
%   \draw[thick][->] (4,1) -- (4,1.8);
%   \draw[thick][->] (14,1.8) -- (14,1);
%   \draw[thick][<->] (7.1,1.3) -- (8.6, 1.3) node[anchor=south]{offloading}-- (10.1,1.3);
%   \node[task7,minimum width=2.8cm,anchor=south west](c) at (4.3,1) {\tiny pre-processing};
%   \node[task7,minimum width=1cm,anchor=south west](d) at (10.1,1) {\tiny post-p.};
%   \node(b2) at (2.7,1.2) {\footnotesize $Receive$ $Results$};
%  \end{tikzpicture} }
%   \caption{An example of computation offloading.}
%   \label{fig:offloading}
% \end{figure}


% {\bf Example 4: Partitioned Scheduling for DAG-Structured Tasks.}
%  \hspace{0.1in}
% To fully utilize the power of multiprocessor systems, a task may be parallelized such that it can be executed simultaneously on several processors to perform independent computation in parallel. We can use a \emph{directed acyclic graph (DAG)} to model the dependency of the subtasks in a sporadic task. Each vertex in the DAG represents a subtask. For example, the DAG structure used in Figure~\ref{fig:example-dac} shows that there are five subtasks of this DAG task, in which the numbers within the vertices are the corresponding execution times. Suppose that we design a partitioned schedule to assign the subtasks with execution times $3,4,$ and $2$ on the first processor and the subtasks with execution times $0.5,$ and $7$ on the second processor to balance the workload on these two processors. As shown in the schedule in Figure~\ref{fig:example-dac}, both processors experience some idle time 
% due to the precedence constraints of the DAG task. Such idle time intervals can also be considered to be suspensions~\cite{fonseca2016response}. 




% \begin{figure}[t]
%   \centering
%     \scalebox{0.8}{
% \def\uxDAG{0.75cm} 
%       \begin{tikzpicture}[x=\uxDAG,auto, thick]
%     \node [draw,circle](t0)at(0,4){$0.5$};
%     \node [draw,circle](t1)at(0,2){$3$};
%     \node [draw,circle](t2)at(2,3){$4$};
%     \node [draw,circle](t3)at(2,2){$7$};
%     \node [draw,circle](t4)at(4,4){$2$};
%     \draw[->] (t3) -- (t4);%kappa 1    
%     \draw[->] (t1) -- (t2);%kappa 1    
%     \draw[->] (t1) -- (t3);%kappa 2
%     \draw[->] (t2) -- (t4);%kappa 3    

%       \begin{scope}[shift={(6.3,2)}]
%        \draw[->] (0,0) -- coordinate (xaxis) (12.5,0) node[anchor=north]{$t$};
%        \node[anchor=east] at (0, 0.3) {Proc. 2};
%        \node[anchor=east] at (0, 1.8) {Proc. 1};
%        \foreach \x in {0,1,...,12}{
%          \draw[-,below](\x,0) -- (\x,-0.1)
%          node[] {\pgfmathtruncatemacro\yi{\x} \yi};
%        }
%          \node[task7, minimum width=0.5*\uxDAG, anchor=south west] at (0, 0){\tiny $.5$};         
%          \node[task7, minimum width=3*\uxDAG, anchor=south west] at (0, 1.5){\footnotesize $3$};
%          \node[task7, minimum width=4*\uxDAG, anchor=south west] at (3, 1.5){\footnotesize $4$};
%          \node[task7, minimum width=7*\uxDAG, anchor=south west] at (3, 0){\footnotesize $7$};
%          \node[task7, minimum width=2*\uxDAG, anchor=south west] at (10, 1.5){\footnotesize $2$};

%          \draw[thick] (7,1.87) -- (8.5, 1.87) node[anchor=south]{suspension}-- (10,1.87);
%          \draw[thick] (7,1.775) -- (8.5, 1.775) node[anchor=south]{}-- (10,1.775);
%          \draw[thick] (7,1.68) -- (8.5, 1.68) node[anchor=south]{}-- (10,1.68);
%          \draw[thick] (0.54,0.37) -- (1.75, 0.37) node[anchor=south]{suspension}-- (3,0.37);
%          \draw[thick] (0.54,0.275) -- (1.75, 0.275) node[anchor=south]{}-- (3,0.275);
%          \draw[thick] (0.54,0.17) -- (1.75, 0.17) node[anchor=south]{}-- (3,0.17);
%          \draw[thick] (10,0.37) -- (11.3, 0.37) node[anchor=south]{suspension}-- (12,0.37);
%          \draw[thick] (10,0.275) -- (11.3, 0.275) node[anchor=south]{}-- (12,0.275);
%          \draw[thick] (10,0.17) -- (11.3, 0.17) node[anchor=south]{}-- (12,0.17);
%          \draw (12,0) -- (12,0.5);
%        \end{scope}
%   \end{tikzpicture}}       
%   \caption{An example of partitioned DAG schedule.}
%   \label{fig:example-dac}
% \end{figure}



\mychapter{Real-Time Sporadic Self-Suspending Task Models}
\label{sec:model}

We now recall the definition of the classic sporadic task model (without self-suspensions)~\cite{Liu_1973,Mok:1983:FDP:888951} and then introduce the main models of self-suspensions. 

The sporadic task model characterizes a task $\tau_i$ as a three-tuple $(C_i,T_i,D_i)$. Each sporadic task $\tau_i$ can release an infinite number of jobs (also called task instances) under the given minimum inter-arrival time (also called period) constraint $T_i$.  Each job released by a sporadic task $\tau_i$ has a relative deadline $D_i$.  That is, if a job of task $\tau_i$ arrives at time $t$, it must (in hard real-time systems), or should (in soft real-time systems) be finished before its absolute deadline at time $t+D_i$, and the next instance of the task must arrive no earlier than time $t + T_i$.
The \emph{worst-case execution time} of task $\tau_i$ is $C_i$. That is, the execution time of a job of task $\tau_i$ is at most $C_i$. The utilization of task $\tau_i$ is defined as $U_i=C_i/T_i$.

Throughout this paper, we will use ${\bf T}$ to denote the %input 
task
set and use $n$ to denote the number of tasks in ${\bf T}$. 

If the relative deadline of each task in ${\bf T}$ is equal to its deadline, then the tasks in ${\bf T}$ are said to have
\emph{implicit deadlines}. If the relative deadline of
each task in ${\bf T}$ is no larger than its period, then the  tasks in ${\bf T}$ have \emph{constrained deadlines}. Otherwise, the tasks in ${\bf T}$ have \emph{arbitrary deadlines}.
In this paper, unless explicitly noted otherwise (for instance in some parts of
\mysectionref{}~\ref{sec:soft-realtime}),
we consider only constrained- and
implicit-deadline task systems.

  
Two main models of self-suspending tasks exist: the \emph{dynamic} self-suspension and \emph{segmented} (or \emph{multi-segment})
self-suspension models. These two models have been recently augmented by hybrid self-suspension models~\cite{DBLP:conf/rtcsa/BruggenHC17}.
An additional model, using a \emph{directed acyclic graph} (DAG) representation of the task control flow, can be 
reduced to an instance of the former two models, for analysis purposes \cite{bletsas:thesis}.

\paragraph{Dynamic Self-Suspension Model:} 

The {dynamic} self-suspension sporadic task model characterizes a task $\tau_i$ as a four-tuple $(C_i,S_i,T_i,D_i)$. Similar to the sporadic task model, $T_i$ denotes the 
minimum inter-arrival time (or period) of $\tau_i$, $D_i$ denotes the relative deadline of $\tau_i$ and $C_i$ is an upper bound on 
the total execution time of each job of $\tau_i$. The new parameter $S_i$ denotes an upper bound on the total suspension time of each job of $\tau_i$.  

The dynamic self-suspension model is convenient when it is not possible to know \textit{a priori} the number and/or the location of self-suspension intervals for a task, \eg, when these may vary for different jobs of the same task.

For example, in the general case, a task may have several possible control flows, where the actual execution path depends on the values
of the program and/or system variables at run-time. Each of those paths may have a different
number of self-suspension intervals. Additionally, during the execution of a job of a task, one control flow may have a self-suspension interval at the beginning of the job and
another one may self-suspend shortly before its completion. Under such circumstances, it is convenient to be able to collapse all these possibilities
by modelling the task according to the dynamic self-suspension model using
just two parameters: the worst-case execution time of the task in consideration and an upper bound for the time spent in self-suspension by any job of the task. %As explained in \mysectionref{}~\ref{sec:dag_model}, this can be done by converting the information provided by a DAG into the dynamic self-suspension task model. 
%Note that these two worst cases may be observed under different control flows.

\paragraph{Segmented Self-Suspension Model:} 

The {segmented} self-suspension sporadic task model extends the four-tuple of the dynamic model by further characterizing the computation segments and suspension 
intervals using an array $(C_{i}^1,S_{i}^1,C_{i}^2,S_{i}^2,\ldots, $ $S_{i}^{m_i-1},C_{i}^{m_i})$. Each job of $\tau_i$ is assumed to be composed of $m_i$ computation segments 
separated by $m_i-1$ suspension intervals. The execution time of the $\ell^{\text{th}}$ computation segment is upper bounded by  $C_{i}^{\ell}$, and the length of the $\ell^{\text{th}}$ suspension interval is upper bounded by $S_{i}^{\ell}$. For a segmented sporadic task $\tau_i$, we have 
$C_i = \sum_{\ell=1}^{m_i} C_i^\ell$ and $S_i=\sum_{\ell=1}^{m_i-1} S_i^\ell$.

The segmented self-suspension model is a natural choice when the code structure of a task exhibits a certain linearity,
\ie, there is a deterministic number of self-suspension intervals interleaved with portions of processor-based code with single-entry
single-exit control-flow semantics. Such tasks can always be modeled according to the dynamic self-suspension
model, but this would discard the information about the constraints in the location of self-suspensions intervals of a job, \ie, in the control flow. The segmented self-suspension model preserves this information, which can be potentially used
to derive tighter bounds on worst-case response times or exploited for designing better scheduling strategies.

\paragraph{Hybrid Self-Suspension Model:} 

The dynamic self-suspension model is very flexible but inaccurate, whilst the segmented self-suspension model is very restrictive but very accurate. The hybrid self-suspension task models proposed in \cite{DBLP:conf/rtcsa/BruggenHC17} assume that in addition to $S_i$, each task $\tau_i$ has at most a known number of $m_i-1$ suspension intervals.  This means that the execution of each job of $\tau_i$ is composed of at most $m_i$ \emph{computation} segments separated by $m_i-1$ \emph{suspension} intervals, similar to the segmented self-suspension model. The sum of the execution times of the computation segments of a job of task $\tau_i$ is at most its WCET $C_i$, while the sum of the lengths of the self-suspension intervals of a job of task $\tau_i$ is at most its worst-case suspension time $S_i$. Depending on the known information, different hybrid self-suspension models were proposed in~\cite{DBLP:conf/rtcsa/BruggenHC17} 
with different
trade-offs between flexibility and accuracy.

\paragraph{DAG-based Self-Suspension Model:} 


In the {DAG-based} self-suspension model~\cite{bletsas:thesis}, each node represents either a self-suspension interval or a computation segment
with single-entry-single-exit control flow semantics. Each possible path from the source node to the sink node
represents a different program execution path. Note that a linear graph is already an instance of the segmented self-suspension model.
An arbitrary task graph can be reduced with some information loss (pessimism) to an instance of the dynamic self-suspension model. 


%, \eg, by using the segmented model for those tasks that can be modeled according to
% it and using the dynamic model for all other tasks.





%
%with 
%the following parameters:
%
%\begin{align} 
%C_i =  &\max_{\forall \varphi} \Big(  \sum_{\ell \in \varphi} C_i^\ell                                      \Big)         \nonumber\\
%S_i =  &\max_{\forall \varphi} \Big(  \sum_{\ell \in \varphi} C_i^\ell + \sum_{\ell \in \varphi} S_i^\ell   \Big)  - C_i  \nonumber
%\end{align}
%
%where $\varphi$ denotes a control flow (path), as a set of nodes traversed~\cite{RTAS-AudsleyB04,bletsas:thesis}.
A simple and safe method is to use
\begin{equation*} 
C_i =  \max_{\forall \varphi} \Big(  \sum_{\ell \in \varphi} C_i^\ell   \Big)        \mbox{   and }
S_i =  \max_{\forall \varphi} \Big(  \sum_{\ell \in \varphi} S_i^\ell   \Big),
\end{equation*}
where $\varphi$ denotes a control flow (path), \ie, a set of nodes traversed during the execution of a job~\cite{RTAS-AudsleyB04,bletsas:thesis}. However, it is unnecessarily pessimistic, since the maximum execution time and maximum self-suspension 
time may be observed in different node paths. A more efficient conversion would use
\begin{align} 
S_i =  &\max_{\forall \varphi} \Big(  \sum_{\ell \in \varphi} C_i^\ell + \sum_{\ell \in \varphi} S_i^\ell   \Big)  - C_i  \nonumber
\end{align}
where $C_i$ is still computed as explained above. We will explain the underlying intuition  (partial modeling of self-suspension as computation, which is a safe transformation) 
in Section~\ref{sec:model-interferred-oblivious} (see also~\cite{RTAS-AudsleyB04,BletsasReport2015}).

\paragraph{Remarks on Self-Suspension Models:} 

Note that all of the above models can additionally be augmented with \emph{lower bounds} for segment execution times and suspension 
lengths; when absent, these are implicitly assumed to be zero.


From the system designer's perspective, the dynamic self-suspension model provides an easy way to specify self-suspending systems 
without considering the control flow surrounding I/O accesses, computation offloading, or synchronization. However, from an analysis perspective, such a 
dynamic model may lead to quite pessimistic results in terms of schedulability since the occurrence of suspensions within a job is 
unspecified. By contrast, if the suspension patterns are well-defined and characterized with known suspension intervals, the 
segmented self-suspension task model is more appropriate.   
Note that it is possible to employ both the dynamic self-suspension model and the segmented self-suspension model simultaneously 
in one task set.
The hybrid self-suspension models can be adopted
with different
trade-offs between flexibility and accuracy.
Further note that the DAG self-suspension model is a representational model without its own scheduling analysis. For analysis purposes, it is converted to an instance of either the dynamic or the segmented self-suspension model, which may then serve as input to existing analysis techniques. 


  


% \mysection{Examples of the DAG Self-Suspension Model}
% The DAG Self-Suspension Model, in a simple case, can represent a program, which after some pre-processing, reaches a conditional branch.
% If the branch is taken, the rest of the computation is done on the processor; else, it is outsourced to a co-processor and the task
% self-suspends until the result is available. 

% Note that the DAG self-suspension model is a representational model without its own scheduling analysis. For analysis purposes, it
% is converted to an instance of either the dynamic or the segmented model, which may then serve as input to existing analysis techniques.
% For this reason, in the rest of this survey, we focus on those two models.

% As we will see in the next section, it is possible for both the dynamic model and the segmented model to be employed simultaneously 
% during the analysis of the same task set: for example, by using the segmented model for those tasks that can be modeled according to 
% it and using the dynamic model for all other tasks.


\mysection{Assumptions and Terminology}

\mysubsection{Scheduling:}

Implicitly, we will assume that the system schedules  jobs in a
\emph{preemptive} manner, unless specified otherwise.  We will mainly focus on
uniprocessor systems; however some results for multiprocessor systems
will be discussed in Section~\ref{sec:multiprocessor-HRT} and
\mysectionref{}~\ref{sec:soft-realtime}. 
We assume that the cost of preemption
has been subsumed into the worst-case execution time of each task. In
uniprocessor systems, \ie, in \mysectionref{}~\ref{sec:review} and
\mysectionref{}~\ref{sec:misconceptions} (except Section~\ref{sec:multiprocessor-HRT}), we will consider both
earliest-deadline-first (EDF) and 
fixed-priority (FP)
scheduling as well as some of their variants. 

Under EDF, 
a task may change its priority at run-time; the highest priority being given to the job (in the ready queue) with the earliest
absolute deadline. Variants of EDF scheduling for self-suspending
tasks have been explored in
\cite{RTSS-ChenL14,Liu_2014,DBLP:conf/ecrts/Devi03,WC16-suspend-DATE,Bruggen16RTNS}.

For fixed-priority scheduling, in general, a task is assigned a
unique priority level, and all the jobs generated by the task have the
same priority level. Examples are rate-monotonic (RM) scheduling
\cite{Liu_1973}, \ie, a task with a shorter period has a
higher-priority level, and deadline-monotonic (DM) scheduling, \ie,
a task with a shorter relative deadline has a higher-priority level.
In this paper, if we consider fixed-priority scheduling, we will also implicitly assume that task $\tau_i$ has higher priority than task $\tau_j$ if $i < j$.
Such task-level fixed-priority scheduling strategies for the self-suspension task models have been explored in
\cite{Raj:suspension1991,RTCSA-KimCPKH95,MingLiRTCSA1994,PH:rtss98,ECRTS-AudsleyB04,RTAS-AudsleyB04,RTCSA-BletsasA05,LR:rtas10,RTSS-KimANR13,LiuChen:rtss2014,huangpass:dac2015,Huang:multiseg,WC16-suspend-DATE,ChenECRTS2016-suspension}.
Moreover, in some results in the literature, \eg,
\cite{RTSS-KimANR13,DBLP:journals/ieicet/DingTT09}, each computation
segment in the segmented self-suspending task model has its own unique
priority level. Such a scheduling
policy is referred to as \emph{segmented fixed-priority scheduling}.

For \emph{hard real-time} tasks, each job should be finished before its
absolute deadline. For \emph{soft real-time} tasks, deadline misses are
allowed. We will mainly focus on hard real-time tasks. 
Soft real-time tasks will be briefly considered in
\mysectionref{}~\ref{sec:soft-realtime}.

\mysubsection{Analysis:}

The \emph{response time} of a job is defined as the difference between its finishing time and its arrival
time. The \emph{worst-case response time} (WCRT) of a real-time task
$\tau_k$ in a task set ${\bf T}$ is defined as an upper bound on the
response times of all the jobs of task $\tau_k \in {\bf T}$ for any
\emph{legal sequence} of jobs of ${\bf T}$. A sequence of jobs of
the task system ${\bf T}$ is a legal sequence if any two consecutive
jobs of task $\tau_i \in {\bf T}$ are separated by \emph{at least}
$T_i$ and the self-suspension and computation behavior are upper
bounded by the defined parameters. The goal of response time analysis is to
analyze the worst-case response time of a certain task $\tau_k$ in the
task set ${\bf T}$ or all the tasks in ${\bf T}$.

A task set ${\bf T}$ is said to be \emph{schedulable} by a scheduling algorithm $\mathcal{A}$ if the worst-case response time of each task 
$\tau_k$ in ${\bf T}$ is no more than its relative deadline $D_k$.
A \emph{schedulability test} for a scheduling algorithm $\mathcal{A}$ is a test checking whether a task set ${\bf T}$ is schedulable with $\mathcal{A}$. There are
two usual types of schedulability tests:
\begin{itemize}
\item Utilization-based schedulability tests. Examples of such tests are the
  utilization bounds by Liu and Layland \cite{Liu_1973} and the hyperbolic bound by Bini et al. \cite{bini2003rate}.
\item Time-demand analysis (TDA) or response time analysis (RTA) \cite{lehoczky-1989}. Several exact tests exist for periodic and sporadic tasks without suspension (\eg, \cite{Liu_1973,spuri_96, goossens1997non, goossens1999feasibility, zhang2009schedulability}). %For fixed priority sporadic tasks without suspension, those tests are based on the critical instant
  %theorem in \cite{Liu_1973} to evaluate the worst-case response time
  %precisely.  
  %That is, the worst-case response time of task $\tau_k$ is
%  the minimum positive $R_k$ such that \gn{this bound is limited to FP with constrained deadlines but the section is more generic than that. We should precisely state in which context this equation is correct}
%  \begin{equation}
%   \label{eq:rta}
%  R_k = C_k+ \sum_{\tau_i \in hp(k)}\ceiling{\frac{R_k}{T_i}} C_i,     
%  \end{equation}
%  where $hp(k)$ is the set of the tasks with higher-priority levels
%  than $\tau_k$.  
\end{itemize}
We consider both types of analyses in this paper.


To solve the computational complexity issues of many scheduling problems in real-time systems, approximation algorithms based on \textit{resource augmentation} with respect to \emph{speedup factors} have attracted much attention.  If an algorithm ${\cal A}$ has a \emph{speedup factor} $\rho$, then any task set that is schedulable (under the optimal scheduling policy) at the original platform speed is also schedulable by algorithm ${\cal A}$ when all the processors have speed $\rho$ times the original platform speed.
%In other words, by taking the negation of the above statement, if an algorithm ${\cal A}$ has a \emph{speedup factor} $\rho$, then it guarantees that \emph{if the schedule derived from the algorithm ${\cal A}$ is not feasible, then the input does not admit a feasible schedule by running at speed $\frac{1}{\rho}$}.


%% Commented due to Cong's comment on 24,11,2015
% It is well known that uncontrolled deferred execution (due to
% self-suspension) can impose a scheduling penalty. The above
% utilization-based schedulability test and time-demand analysis
% have to be revisited and extended to handle the scheduling penalties
% resulting from self-suspending behavior.

\mysubsection{Platform:}

Most of this paper focuses on single processor systems. However, the multiprocessor case is discussed in Section~\ref{sec:multiprocessor-HRT}~and~\mysectionref{}~\ref{sec:soft-realtime}. When addressing the scheduling of tasks on multiprocessor systems, we distinguish between two major categories of multiprocessor real-time schedulers:  (\textit{i}) partitioned scheduling and (\textit{ii}) global scheduling. 

Under partitioned scheduling, tasks are statically partitioned among processors, \ie, each task is bound to execute on a specific processor and never migrates to another processor. An often used multiprocessor partitioned scheduling algorithm is partitioned EDF (P-EDF), which applies EDF on each processor individually.
Partitioned fixed-priority (P-FP) scheduling is another widespread
choice in practice due to the wide support in industrial standards
such as AUTOSAR, and in many RTOSs like VxWorks, RTEMS, ThreadX, \etc
Under P-FP scheduling, each task has a fixed-priority level and is statically assigned to a specific processor, and each processor is scheduled independently as a uniprocessor.  
%Under EDF, jobs with earlier deadlines have higher priority. 
 In contrast to partitioned scheduling, under global scheduling, jobs that are ready to be executed are dynamically dispatched to available processors, \ie, jobs are allowed to migrate from one processor to another at any time. For example, global EDF (G-EDF) is a global scheduling algorithm under which jobs are EDF-scheduled using a single ready queue.
%(at least conceptually; actual implementations may use several synchronized queues).  

%\mysection{Multiprocessor Scheduling}

%\gn{it looks weird to have a subsection specifically for multicore. Why do we need to insists on multicore than on singlecore. In fact this subsection looks like a copy paste from the introduction of another paper that has nothing to do with this one. We must find a way to integrate this in the general flow of this chapter.}

%Two major categories of multiprocessor real-time schedulers are  (\textit{i}) partitioned scheduling and (\textit{ii}) global scheduling. 
%Under partitioned scheduling, tasks are statically partitioned among processors, \ie, each task is bound to execute on a specific processor and never migrates to another processor. Different processors can apply different scheduling algorithms. An often used multiprocessor partitioned scheduling algorithm is partitioned earliest-deadline-first (P-EDF), which applies EDF on each processor individually.
%Partitioned fixed-priority (P-FP) scheduling is another widespread
%choice in practice due to the wide support in industrial standards
%such as AUTOSAR, and in many RTOSs like VxWorks, RTEMS, ThreadX, \etc
%Under P-FP scheduling, each task has a fixed-priority level and is statically assigned to a specific processor, and each processor is scheduled independently as a uniprocessor.  
%Under EDF, jobs with earlier deadlines have higher priority. 
% In contrast to partitioned scheduling, under global scheduling,  ready jobs are dynamically dispatched to available processors, \ie, jobs are allowed to migrate from one processor to another at any time. For example, global EDF (G-EDF) is a global scheduling algorithm under which jobs are EDF-scheduled using a single ready queue (at least conceptually; actual implementations may use several synchronized queues).  

%We will discuss the multiprocessor scenarios in Section~\ref{sec:multiprocessor-HRT}~and~\mysectionref{}~\ref{sec:soft-realtime}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "JRTS/JRTS.tex"
%%% End:


  
  
  
