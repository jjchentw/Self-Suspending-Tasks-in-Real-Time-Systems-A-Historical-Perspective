\section{General Design and Analysis Strategies}
\label{sec:review}

This section reviews existing solutions for scheduling and analyzing the schedulability of self-suspending task models. We will first describe the commonly adopted strategies in those solutions. The  strategies are generally correct, but the analysis has to be done carefully.  In the next section, we will explain some of misconceptions used in the literature by giving concrete reasons and some counterexamples to explain why such misconceptions may lead to over-optimistic analysis. 

For self-suspending sporadic task systems, while executing, a job may suspend itself or even must suspend itself in the segmented self-suspension model. While a job suspends, the scheduler removes the job from the ready queue. Such suspensions should be well characterized and the resulting workload interference should be well quantified to analyze schedulability.

\emph{We will implicitly assume uniprocessor systems in this section}, except in Section~\ref{sec:multiprocessor-HRT}. There are some common strategies to characterize and quantify the impact due to self-suspensions. We categorize these methods as follows:
\begin{itemize}
\item \textbf{Convert All Self-Suspension into Computation} (in
  Section~\ref{sec:oblivious}): This method converts all self-suspending time
  into computation time. Such a strategy is also referred to as
  \emph{suspension-oblivious} analysis in the literature.
\item \textbf{Convert Higher-Priority Tasks into Ordinary Sporadic Tasks} (in
  Section~\ref{sec:high-as-sporadic}): This method converts all the
  higher-priority tasks into periodic tasks without self-suspensions,
  except the lowest-priority task under FP scheduling. This also leads
  to one of the most fundamental problems of self-suspending task
  systems: \emph{How can we efficiently analyze the worst-case response time
  of a self-suspending task $\tau_k$ as the lowest-priority task in
  the task system, when all the other higher priority tasks are
  ordinary sporadic real-time tasks?}
\item \textbf{Quantify Additional Interference due to
    Self-Suspensions} (in
  Section~\ref{sec:method-quantify-interference}): It is well-known
  that uncontrolled deferred executions (due to self-suspension) can
  impose scheduling penalty. The methods in this category aim to
  quantify the additional interference due to self-suspending
  behaviour while performing schedulability tests or worst-case
  response time analysis.

\item \textbf{Treat Self-Suspension Segments as Idling or Computation
    Alternatively} (in Section~\ref{sec:suspend-or-not}): When we
  analyze the worst-case response time of a segmented self-suspending
  task $\tau_k = ((C_k^1, S_k^1, C_k^2), T_k, D_k)$, there are two
  options (1) convert $S_k^1$ into computation or (2) treat $S_k^1$ as
  if the processor idles. Due to the lack of the critical instant
  theorem for such a problem under fixed-priority scheduling, these two treatments
  do not dominate each other and have different benefits, to be
  discussed in Section~\ref{sec:suspend-or-not}.

\item \textbf{Enforce Periodic Behaviour by Release Time Enforcement} (in
  Section~\ref{sec:periodic-enforce}): The uncontrolled
  self-suspending behaviour can result in additional interference.  To
  alleviate the impact on the additional interference, the methods in
  this category aim to control the release time of the computation
  segments to enforce periodic release behaviour.
\end{itemize}


To demonstrate how the scheduling algorithms and the schedulability tests work in existing approaches, we will mainly use the following tasks in Table~\ref{table:dynamic-example} and Table~\ref{table:static-example} in this section. For demonstrating the worst-case response time analysis, we leave some relative deadline with "?" and period $\infty$. Specifically, we will use task set ${\bf T}_1 = \setof{\tau_1, \tau_2, \tau_3}$, ${\bf T}_2 = \setof{\tau_1, \tau_2, \tau_3, \tau_4}$, ${\bf T}_3 = \setof{\tau_\alpha, \tau_\beta, \tau_\gamma}$ in our examples. Unless specified otherwise, we will assume that these three example task sets are scheduled under rate-monotonic (RM) scheduling in this section. 

\ifpaper
\begin{table}[t]
\else
\begin{table} 
\fi
\centering
    \begin{tabular}{|c|c|c|c|c|}
 \hline
        & $C_i$ &  $S_i$&  $D_i$ & $T_i$\\ 
        \hline
        $\tau_\alpha$ & 1 & 0 &  2 & 2\\ 
        $\tau_\beta$ &  5&  5& 20 & 20 \\ 
        $\tau_\gamma$ & 1 & 0  & ? & $\infty$ \\ 
        \hline
    \end{tabular} 
    \caption{Examples for dynamic self-suspending tasks to be used in Section~\ref{sec:review}.}
    \label{table:dynamic-example}
\end{table}

\ifpaper
\begin{table}[t]
\else
\begin{table} 
\fi
\centering
    \begin{tabular}{|c|c|c|c|}
 \hline
        & $(C_i^1, S_i^2, C_i^2)$ &  $D_i$ & $T_i$\\ 
        \hline
        $\tau_1$ & (2, 0, 0) &  5 & 5\\ 
        $\tau_2$ &  (2, 0, 0) & 10 & 10 \\ 
        $\tau_3$ & (1, 5, 1) & 15  & 15\\
        $\tau_4$ & (3, 0, 0) & ? & $\infty$\\
        \hline
    \end{tabular} 
    \caption{Examples for segmented self-suspending tasks to be used in Section~\ref{sec:review}.}
    \label{table:static-example}
\end{table}




\subsection{Convert All Self-Suspension into Computation}
\label{sec:oblivious}

 This is the simplest and the most pessimistic strategy. It converts all self-suspending time into computation time. Such a strategy is also referred to as \emph{suspension-oblivious} analysis in the literature. That is, we can consider the execution time of task $\tau_i$ to be $C_i+S_i$. After this conversion, we only have ordinary sporadic real-time tasks. Therefore, all the existing results for sporadic task systems can be applied. The proof can be done with the following simple interpretation: The suspension of a job may make the processor idle. If two jobs suspend at the same time and the processor idles in a certain time interval in the actual schedule, it can be imagined that one of these two jobs has shorter execution time (than its worst-case execution time $C_i+S_i$). Such earlier completion does not affect the schedulability analysis. Therefore, putting $C_i+S_i$ as the worst-case execution time for every task $\tau_i$ is a very safe analysis for both dynamic- and static-scheduling policies.  Such an approach has been widely used as the baseline of more accurate analyses in the literature.

With this schedulability test, it is easy to see that none of the three example task sets ${\bf T}_1$, ${\bf T}_2$, ${\bf T}_3$ can be classified as feasible since $\frac{1}{2} + \frac{5+5}{20} + \frac{1}{D_\gamma} > 1$ and $\frac{2}{5}+\frac{2}{10}+\frac{1+5+1}{15} > 1$.

\subsection{Convert Higher-Priority Tasks into Ordinary Sporadic Tasks}
\label{sec:high-as-sporadic} 

In fixed-priority scheduling, when we analyze the schedulability of a task $\tau_k$, we can convert the higher-priority self-suspending tasks into ordinary sporadic tasks by treating their suspension time as computation. That is, a higher-priority task $\tau_i$ (higher than task $\tau_k$) now has worst-case execution time $C_i+S_i$. This simplifies the analysis. After converting, we only have one self-suspending task left as the lowest-priority task in the system.  
% Such a conversion is useful for analyzing the segmented self-suspending task model. However, such a conversion is not very useful for analyzing dynamic self-suspending task models, since we have to consider the worst case that the self-suspension of task $\tau_k$ makes the processor idle. Therefore, we also have to convert $\tau_k$'s self-suspension into computation. This results in identical analysis as the suspension-oblivious analysis in Section~\ref{sec:oblivious} by converting self-suspension into computation for all the tasks. 

With the conversion, the fundamental problem is to analyze the worst-case response time of a self-suspending task $\tau_k$ as the lowest-priority task in the task system, when all the other higher priority tasks are ordinary sporadic real-time tasks. One simple strategy is to analyze the worst-case response time $R_k^j$ for each computation segment $C_k^j$. The schedulability test of task $\tau_k$ then is to simply verify whether $R_k^{m_k} + \sum_{j=1}^{m_k-1} R_k^j + S_k^j \leq D_k \leq T_k$. We will use task set ${\bf T}_1$ as an example. The worst-case response times of $C_3^1=1$ and $C_3^2=1$ in ${\bf T}_1$ are both clearly $5$ by using standard the time demand analysis (TDA). Therefore, we know that the worst-case response time of task $\tau_3$ in ${\bf T}_1$ is at most $15$.

The above test can be fairly pessimistic, especially when the suspension times are short. Imagine that we change $S_3^1$ from $5$ to $1$. The above analysis still considers that both computation segments suffer from the worst-case interference from the two higher-priority tasks and returns $11$ as the (upper bound of the) worst-case response time. For this new configuration, if we greedily convert the suspension into computation and use TDA analysis, we can conclude that the worst-case response time is at most $9$. 

Therefore, it can be more accurate if the higher-priority interference is analyzed more precisely. However, it has to be done carefully. 
The problem with only one self-suspending task $\tau_k$ as the lowest-priority task has been specifically studied in \cite{LR:rtas10,ecrts15nelissen}. The strategy is to find the critical instant of task $\tau_k$, at which, considering the state of the system, an execution request for $\tau_k$ will generate the largest response time. It would be helpful if such a critical instant were easy to find.
Unfortunately, the analysis in \cite{LR:rtas10} is flawed. We will explain the reasons and the solutions to fix the flaws in Section~\ref{sec:wrong-critical}.




\subsection{Quantify Additional Interference due to Self-Suspensions}
\label{sec:method-quantify-interference}

 Suspensions may result in greater interference by higher-priority jobs to interfere with  respect to a lower-priority job. The strategies here convert the suspension time of a job of task $\tau_k$ under analysis into computation. Suppose that the job under analysis arrives at time $t_k$. The other higher-priority jobs except the job under analysis are considered to (possibly) have self-suspensions. This is the completely opposite strategy to the previous strategy in Section~\ref{sec:high-as-sporadic}. Since a higher-priority self-suspending job may suspend itself before $t_k$ and resume after $t_k$, the self-suspending behaviour of a task $\tau_i$ can be considered to bring \emph{at most} one \emph{carry-in} job to be \emph{partially} executed after $t_k$ if $D_i \leq T_i$. As we have converted task $\tau_k$'s self-suspension time into computation, the finishing time of the job of task $\tau_k$ is the earliest moment after $t_k$ such that the processor idles. 

\jj{a figure}

Here, we implicitly assume that all the higher-priority tasks are already verified to meet their \emph{constrained deadlines}, i.e., $D_i \leq T_i$ for a higher-priority task $\tau_i$.
\begin{itemize}
\item In the \emph{dynamic self-suspending task model}, the above analysis implies that the higher-priority jobs arrived after time $t_k$ \emph{should not} suspend themselves to create the maximum interference. Therefore, suppose that the first arrival time of task $\tau_i$ after $t_k$ is $t_i$, i.e., $t_i \geq t_k$. Then, the demand of task $\tau_i$ released at time $t \geq t_i$ is $\ceiling{\frac{t-t_i}{T_i} } C_i$. So, we just have to account for the demand of the carry-in job of task $\tau_i$ executed between $t_k$ and $t_i$. The workload of the carry-in job can be up to $C_i$ (due to the assumption $D_i \leq T_i$), but can also be characterized in a more precise manner. The approaches in this category are presented in \cite{huangpass:dac2015,LiuChen:rtss2014} by greedily counting $C_i$ in the carry-in job. 
Jane W.S. Liu in her book \cite[Page 164-165]{Liu:2000:RS:518501} presents an approach to quantify the higher-priority tasks by setting up the \emph{blocking time} induced by self-suspensions. In her analysis, a job of task $\tau_k$ can suffer from the \emph{extra delay} due to self-suspending behavior as a factor of blocking time, denoted as $B_k$, as follows: (1) The blocking time contributed from task $\tau_k$ is $S_k$. (2) A higher-priority task $\tau_i$ can only block the execution of task $\tau_k$ by at most $b_i=min(C_i, S_i)$ time units. In the book \cite{Liu:2000:RS:518501}, the blocking time $B_k=S_k+\sum_{i=1}^{k-1} b_i$ is then used to perform utilization-based analysis for rate-monotonic scheduling. However, there was no proof in the book. Fortunately, the recent report from Chen et al. \cite{ChenHuangNelissen} has provided a proof to support the correctness of the above method in \cite[Page 164-165]{Liu:2000:RS:518501}.

We use task set ${\bf T}_3$ to illustrate the above analysis in \cite[Page 164-165]{Liu:2000:RS:518501}, see Table~\ref{table:dynamic-example}. In this case, $b_\beta$ is $5$. Therefore, $B_\gamma = 5$. So, the worst-case response time of task $\tau_\gamma$ is upper bounded by the minimum $t$ with $t=B_\gamma+C_\gamma+\ceiling{\frac{t}{T_\alpha}} C_\alpha +\ceiling{\frac{t}{T_{\beta}}} C_\beta = 6+\ceiling{\frac{t}{2}} 1 +\ceiling{\frac{t}{20}} 5$. The above equality holds when $t=32$. Therefore, the worst-case response time of task $\tau_{\gamma}$ in ${\bf T}_3$ is upper bounded by $32$.
\vspace{0.1in}


Another way to quantify the impact is to model the impact of the carry-in job by using the concept of \emph{jitter}. If the jitter of task $\tau_i$ to model self-suspension is $J_i$, then, the demand of task $\tau_i$ released from $t_i-T_i$ up to time $t+t_k$  (i.e., the demand that can be executed from $t_k$ to $t_k+t$) is $\ceiling{\frac{t+J_i}{T_i} } C_i$. A safe way is to set $J_i$ to $T_i$, which can be imagined as a pessimistic analysis by assuming that the carry-in job of task $\tau_i$ has execution time $C_i$ and the release time $t_i$ is $t_k$. A more precise way to quantify the jitter is to use the worst-case response time of a higher-priority task $\tau_i$. Therefore, we can set the jitter $J_i$ of task $\tau_i$ to $D_i-C_i$ \cite{huangpass:dac2015,Raj:suspension1991} or  $R_i-C_i$,
where $R_i$ is the worst-case response time of a higher-priority task $\tau_i$. \footnote{The case with $J_i = R_i-C_i$ is not presented in \cite{huangpass:dac2015} explicitly, since the focus in \cite{huangpass:dac2015} is to provide priority assignments. However, the proof can be directly applied to conclude that setting the jitter as $R_i-C_i$ is safe as long as $R_i \leq T_i$.}

We use task set ${\bf T}_3$ to illustrate the above analysis in \cite{huangpass:dac2015}. In this case, $J_\beta$ is $20-5=15$. So, the worst-case response time of task $\tau_\gamma$ is upper bounded by the minimum $t$ with $t=C_\gamma+\ceiling{\frac{t}{T_\alpha}} C_\alpha +\ceiling{\frac{t+15}{T_{\beta}}} C_\beta = 1+\ceiling{\frac{t}{2}} 1 +\ceiling{\frac{t+15}{20}} 5$. The above equality holds when $t=22$. Therefore, the worst-case response time of task $\tau_{\gamma}$ in ${\bf T}_3$ is upper bounded by $22$.


There have been some flawed analyses in the literature \cite{ECRTS-AudsleyB04,RTAS-AudsleyB04,RTCSA-KimCPKH95} which quantify the jitter of task $\tau_i$ by setting $J_i$ to $S_i$. We will explain later in Section \ref{sec:wrong-jitter-dynamic} why setting $J_i$ to $S_i$ is in general too optimistic. 

\item In the \emph{segmented self-suspending task model}, we can simply ignore the segmentation structure of computation segments and suspension intervals and directly apply all the strategies for dynamic self-suspending task models. However, the analysis will become pessimistic. This is due to the fact that the segmented-suspensions are not completely dynamic. The static suspension patterns result in also certain (more predictable) suspension patterns. However, characterizing the worst-case suspending patterns of the higher priority tasks to quantify the additional interference under segmented self-suspending task model is not easy. Similarly, one possibility is to characterize the worst-case interference in the carry-in job of a higher-priority task $\tau_i$ by analyzing its self-suspending pattern, as presented in \cite{Huang:multiseg}. Another possibility is to  quantify the interference by modeling it with a jitter term, as presented in \cite{RTCSA-BletsasA05}. We will explain later in Section~\ref{sec:wrong-jitter-segmented} why the quantification of the interference in \cite{RTCSA-BletsasA05} is incorrect. {\bf Michael's paper in RTSS1998}.
\end{itemize}


Let's use task set ${\bf T}_2$ to illustrate how the schedulability tests work. \jj{leave to Kevin and Michael.}
 
\subsection{Treat Self-Suspension Segments  as Suspension or
  Computation Alternatively}
\label{sec:suspend-or-not} 

 Greedily converting the suspension time of a job of task $\tau_k$ under analysis into computation can also become very pessimistic if $S_k$ is much larger than $C_k$. However, the decision to convert a task $\tau_k$ has to be done carefully. Now, we can consider a simple example to analyze the worst-case response time of task $\tau_k = ((C_k^1, S_k^1, C_k^2), T_k, D_k)$. We can have two options:
\begin{itemize}
\item Convert $S_k^1$ into computation, and then apply the above analysis by considering that task $\tau_k$ has execution time $C_k^1+S_k^1+C_k^2$. We simply have to verify whether the worst-case response time is no more than $D_k$.
\item Treat each of the computation segments of task $\tau_k$ individually by applying the worst-case higher-priority interference, regardless of its previous computation segments. We need to verify if the suspension time $S_k$ plus sum of the worst-case response time of all the computation segments of task $\tau_k$ is no more than $D_k$. 
\end{itemize}
The benefit of the former approach is due to that it only pessimistically counts the additional higher-priority interference once. However, it also suffers from the pessimism by converting $S_k^1$ into computation. The benefit of the latter approach is due to the fact that the suspension time is not over-counted as computation. However, it also over-counts the carry-in workload since every computation segment may have to pessimistically count the worst-case workload of the carry-in jobs. Both of these two approaches are adopted in the literature \cite{ecrts15nelissen,Huang:multiseg,RTCSA-BletsasA05}. They can be both applied and the better result is returned.

The example in Section~\ref{sec:high-as-sporadic} when $S_3^1$ is $1$ has demonstrated the difference of the above two difference cases. 

\subsection{Enforce Periodic Behaviour by Release Time Enforcement}   
\label{sec:periodic-enforce}

Self-suspension can cause substantial schedulability degradation. To alleviate the impact on additional interference due to self-suspension, one possibility is to enforce the periodic behaviour by enforcing the release time of the computation segments. There are two categories of such enforcement. 
  \begin{itemize}
  \item {\it Use period enforcement}: Rajkumar \cite{Raj:suspension1991} proposes a \emph{period enforcer} algorithm to handle the impact of uncertain releases (such as self-suspensions). In a nutshell, the period enforcer algorithm artificially increases the length of certain suspensions whenever a task's activation pattern carries the risk of inducing undue interference in lower-priority tasks. By \cite{Raj:suspension1991}, the period enforcer algorithm ``forces tasks to behave like ideal periodic tasks from the scheduling point of view with no associated scheduling penalties''. The period enforcer is revisited by Chen and Brandenburg \cite{ChenBrandenburg}, with the following three observations:
\begin{enumerate}
	\item period enforcement can be a cause of deadline misses in self-suspending tasks sets that are otherwise schedulable;
	\item with current techniques, schedulability analysis of the period enforcer algorithm requires a task set transformation that is subject to exponential time complexity; and 	
        \item the period enforcer algorithm is incompatible with all existing analyses of suspension-based locking protocols, and can in fact cause ever-increasing suspension times until a deadline is missed.
\end{enumerate}
  \item {\it Set a constant offset to constrain the release time of a computation segment}: Suppose that the offset for the $j$-th computation segment of task $\tau_i$ is $\phi_i^j$. This means that the $j$-th computation segment of task $\tau_i$ is released only at time $r_i+\phi_i^j$, in which $r_i$ is the arrival time of a job of task $\tau_i$. With the enforcement, each computation segment can be represented by a sporadic task with a period $T_i$, a WCET $C_i^j$, and a relative deadline $\phi_{i,j+1}-\phi_i^j-S_i^j$. (Here, $\phi_{i,m_i+1}$ is set to $D_i$.) Such approaches have been presented in \cite{RTSS-KimANR13,LR:rtas10,RTSS-ChenL14}. The method in \cite{RTSS-ChenL14} is a simple and greedy solution for implicit-deadline self-suspending task systems with at most one self-suspension interval per task. It assigns the phase $\phi_i^2$ always to $\frac{T_i+S_i^1}{2}$ and the relative deadline of the first computation segment of task $\tau_i$ to $\frac{T_i-S_i^1}{2}$. This is the first method in the literature with \emph{speedup factor} guarantees by using the revised relative deadline for earliest-deadline-first scheduling. 


The methods in \cite{RTSS-KimANR13,DBLP:journals/ieicet/DingTT09} assign each computation segment a fixed-priority level and a phase. Unfortunately,  in \cite{RTSS-KimANR13,DBLP:journals/ieicet/DingTT09}, the schedulability tests are not correct, and the proposed mixed-integer linear programming \cite{RTSS-KimANR13} is unsafe for worst-case response time guarantees. 

\item The slack enforcement in \cite{LR:rtas10} intends to create periodic execution enforcement for self-suspending tasks so that a self-suspending task behaves like an ideal periodic task.  However, the presented methods in \cite{LR:rtas10} require more rigorous proofs to support the correctness. The proof of the key lemma of the slack enforcement mechanism in \cite{LR:rtas10} is incomplete. We will revisit this issue in Section~\ref{sec:open-issues-existing}.
  \end{itemize}



\subsection{Multiprocessor Scheduling for Self-Suspending Tasks}
\label{sec:multiprocessor-HRT}
  
The first suspension-aware worst-case response time analysis for dynamic self-suspending sporadic tasks in a multiprocessor platform is presented in \cite{DBLP:conf/ecrts/LiuA13}. 
The studied scheduling scheme is global scheduling, in which the jobs can be executed on any of the given $M$ identical processors. The analysis in \cite{DBLP:conf/ecrts/LiuA13} is mainly based on the existing results in the literature for global fixed-priority scheduling and dynamic-priority scheduling for ordinary sporadic task systems without self-suspensions. Unfortunately, the schedulability test provided in \cite{DBLP:conf/ecrts/LiuA13} for global fixed-priority scheduling has two flaws: (1) the calculation in Lemma 3 in \cite{DBLP:conf/ecrts/LiuA13}  to calculate the workload bound is unsafe, and (2) it is too optimistic to claim that there are at most $M-1$ carry-in jobs in the analysis interval. The first flaw in (1) can be fixed by using a safe upper bound. The second flaw in (2) is due to the inherited flaw from \cite{DBLP:conf/rtss/GuanSYY09}, in which the flaw has been pointed out in \cite{sun2014improving,DBLP:conf/rtns/HuangC15}, and the patched solutions are also provided in \cite{sun2014improving,DBLP:conf/rtns/HuangC15}. Therefore, by adopting the analysis from \cite{DBLP:conf/rtns/HuangC15}, which is consistent with the analysis in \cite{DBLP:conf/ecrts/LiuA13}, the flaw can be easily fixed. Please refer to \cite{erratu-cong-anderson}.
  
Chen et al. \cite{ChenHLRTSS2015} studied global rate-monotonic scheduling in multiprocessor systems, including dynamic self-suspending tasks. The proposed utilization-based schedulability analysis  in \cite{ChenHLRTSS2015} can be easily extended to handle constrained-deadline task systems and any given fixed-priority assignment.
  
  
  
  
  
  
  
  
  


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "JRTS/JRTS.tex"
%%% End:


  