\section{Self-Suspending Sporadic Real-Time Task Models}
\label{sec:model}
  
Self-suspending tasks can be classified into two models: \emph{dynamic} self-suspension and \emph{segmented} (or \emph{multi-segment}) self-suspension models. 
The dynamic self-suspension sporadic task model characterizes each
task $\tau_i$ as a $4$-tuple $(C_i,S_i,T_i,D_i)$: $T_i$ denotes the minimum inter-arrival time (or period) of $\tau_i$, $D_i$ is the relative deadline,
$C_i$ denotes the upper bound on total execution time of each job of $\tau_i$,
and $S_i$ denotes the upper bound on total suspension time of each job of $\tau_i$.  In addition to the above $4$-tuple, the segmented sporadic task model further 
characterizes the computation segments and suspension intervals as an array
$(C_{i}^1,S_{i}^1,C_{i}^2,S_{i}^2,...,S_{i}^{m_i-1},C_{i}^{m_i})$, composed of $m_i$ computation segments separated by $m_i-1$ suspension intervals. 

Each sporadic task $\tau_i$ can release an infinite number of jobs
(also called task instances) under the given minimum inter-arrival
time (temporal) constraint $T_i$ (also called period).  That is, if a
job of task $\tau_i$ arrives at time $t$, it should be finished before
its absolute deadline $t+D_i$, and  the next instance of
the task must arrive no earlier than $t$ plus the minimum
inter-arrival time, i.e., $t + T_i$.

From the system designer's perspective, the dynamic self-suspension model provides an easy way to specify self-suspending systems without considering the juncture of I/O access, computation offloading, or synchronization. 
However, from the analysis perspective, such a  dynamic model leads to quite pessimistic results in terms of schedulability since the location of suspensions within a job is oblivious. Therefore, if the suspending patterns are well-defined and characterized with known suspending intervals, the multi-segment self-suspension task model is more appropriate.   


Throughout this paper, we will use ${\bf T}$ to denote the input task
set and use $n$ to denote the number of tasks in ${\bf T}$. The
utilization of task $\tau_i$ is defined as $U_i=C_i/T_i$.

If the relative deadline $D_k$ of task $\tau_k$ in ${\bf T}$ is always
equal to the period $T_k$, such a task set ${\bf T}$ is an
\emph{implicit-deadline} task set. If the relative deadline $D_k$ of
task $\tau_k$ in ${\bf T}$ is always no more than the period $T_k$,
such a task set ${\bf T}$ is called a \emph{constrained-deadline} task
set. Otherwise, such a task set is called an \emph{arbitrary-deadline}
task set. We will implicitly consider mainly constrained-deadline and
implicit-deadline task systems in most places, except some parts in
Section~\ref{sec:soft-realtime}.

\subsection{Examples of Dynamic Self-Suspension Model} 
  \textit{different program paths}
  
  \textit{self-suspension due to synchronizations}
  
  etc.
  
\subsection{Examples of Segmented Self-Suspension Model} 
  \textit{static execution patterns}
  
  \textit{multiprocessor synchronization with critical sections}
 
  etc.
  
\subsection{Terminologies and Notations for Scheduling}

Implicitly, we will assume that the system schedules the jobs in a
\emph{preemptive} manner, unless specified.  The cost of preemption
has been subsumed into the worst-case execution time of each task. In
uniprocessor systems, i.e., Section~\ref{sec:review} and
Section~\ref{sec:misconceptions} (except Section~\ref{sec:multiprocessor-HRT}), we will consider both
dynamic-priority scheduling and fixed-priority (FP)
scheduling. A task changes its priority levels in dynamic-priority
scheduling during run-time. One well-known dynamic-priority scheduling
is the earliest-deadline-first (EDF) scheduling, which gives
highest-priority to the job (in the ready queue) with the earliest
absolute deadline. Variances of EDF scheduling for self-suspending
tasks have been explored in
\cite{RTSS-ChenL14,Liu_2014,DBLP:conf/ecrts/Devi03,WC16-suspend-DATE}.

For fixed-priority scheduling, in general, a task is assigned to a
unique priority level, and all the jobs generated by the task have the
same priority level. Examples are rate-monotonic (RM) scheduling
\cite{Liu_1973}, i.e., the task with a shorter period has a
higher-priority level, and deadline-monotonic (DM) scheduling, i.e.,
the task with a shorter relative deadline has a higher-priority level.
This has been explored in
\cite{Raj:suspension1991,RTCSA-KimCPKH95,PH:rtss98,ECRTS-AudsleyB04,RTAS-AudsleyB04,RTCSA-BletsasA05,LR:rtas10,RTSS-KimANR13,LiuChen:rtss2014,huangpass:dac2015,Huang:multiseg,WC16-suspend-DATE}.
Moreover, in some results in the literature, e.g.,
\cite{RTSS-KimANR13,DBLP:journals/ieicet/DingTT09}, each computation
segment in the segmented self-suspending task model has its own unique
priority level. Therefore, all the subjobs generated by the
computation segment have the same priority level. Such a scheduling
policy is referred to as \emph{segmented fixed-priority scheduling}.

For hard real-time tasks, all the jobs should be finished before its
absolute deadline. For soft real-time tasks, deadline misses are
possible. We will mainly focus on hard real-time tasks. The case with
soft real-time tasks will be specifically explored in
Section~\ref{sec:soft-realtime}.

The response time of a job is its finishing time minus its arrival
time.  The worst-case response time (WCRT) of a real-time task
$\tau_k$ in a task set ${\bf T}$ is defined as an upper bound of the
response times of all the jobs of task $\tau_k \in {\bf T}$ for any
\emph{legal sequence} of the jobs of ${\bf T}$. A sequence of jobs of
the task system ${\bf T}$ is a legal sequence if any two consecutive
jobs of task $\tau_i \in {\bf T}$ are separated by \emph{at least}
$T_i$ and the self-suspending and computation behaviour are upper
bounded by the defined parameters. The response time analysis is to
analyze the worst-case response time of a certain task $\tau_k$ in the
task set ${\bf T}$ or all the tasks in ${\bf T}$.

A \emph{schedulability test} of a scheduling algorithm is a test to
verify whether its resulting worst-case response time of each task
$\tau_k$ in ${\bf T}$ is no more than its relative deadline $D_k$. For
the ordinary sporadic task systems without self-suspension, there are
two usual types of schedulability tests for fixed-priority scheduling
algorithms:
\begin{itemize}
\item Utilization-based schedulability test: This includes the
  utilization bound from Liu and Layland \cite{Liu_1973}  and the
  hyperbolic bound from Bini et al. \cite{bini2003rate}.
\item Time-demand analysis (TDA): This is based on the critical instant
  theorem in
  \cite{Liu_1973} to evaluate the worst-case response time
  precisely. That is, the worst-case response time of task $\tau_k$ is
  the minimum positive $R_k$ such that
  \[
  R_k = C_k+ \sum_{\tau_i \in hp(k)}\ceiling{\frac{R_k}{T_i}} C_i, 
  \]
  where $hp(k)$ is the set of the tasks with higher-priority levels
  than $\tau_k$.  
\end{itemize}

It is well known that uncontrolled deferred execution (due to
self-suspension) can impose a scheduling penalty. The above
utilization-based schedulability test and the time-demand analysis
have to be revisited and extended to handle the scheduling penalties
resulting from the self-suspending behaviour.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "JRTS/JRTS.tex"
%%% End:


  
  
  